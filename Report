Machine Learning
Fake News Detection

Project By :
Vaishnavi Gaikwad - 2467 (C22020221468)
Vaishnavi Mundada - 2468 (C22020221469)

Problem statement
Build a project of detecting fake news so that you can easily make a difference between real and fake news.
Introduction
Fake news encapsulates pieces of news that may be hoaxes and is generally spread through social media and other online media. This is often done to further or impose certain ideas and is often achieved with political agendas. Such news items may contain false and/or exaggerated claims and may end up being viralised by algorithms, and users may end up in a filter bubble.
Dataset Information
We have used 2 datasets in this project downloaded from Kaggle.
True.csv: In the true CSV file we have 4 columns representing the title, text content, subject, and date of a news release. There are 21418 rows containing true data so we have enough data to check the accuracy by using the confusion matrix.
Fake.csv: In the fake CSV file we have 4 columns representing the title, text content, subject, and date of a news release. There are 23503 rows containing false data which are used to train and make predictions.


About the Project
In this project, we have imported 
panda and numpy for numeric operations.
Matplotlib for visual representations.
Seaborn for data visualization.
Sklearn for feature extraction, preprocessing data,  accuracy check, training, and testing data.
The project is based on the Decision Tree Algorithm. 
Firstly we import the CSV files and the data is preprocessed.
In data preprocessing we add a new column ‘Target’ in both the datasets with entries true for the true.csv file and false for the fake.csv file.
By using panda concat, we combine both the datasets and then shuffle them by sklearn.shuffle
Now, we remove all the unnecessary columns like date and title. Also, remove all the punctuations and convert data to lowercase for easy and optimized processing of data.
We need to remove all the stop words from this processed data by using nltk library.
Classify the data into true and false based on the subject and make visual representations for better understanding.
By using nltk we can count the frequency of a word and show them graphically.
The Natural Language Toolkit (NLTK) is a platform used for building Python programs that work with human language data for applying in statistical natural language processing (NLP). It contains text processing libraries for tokenization, parsing, classification, stemming, tagging and semantic reasoning.
TF (Term Frequency): The number of times a word appears in a document is its Term Frequency. A higher value means a term appears more often than others, and so, the document is a good match when the term is part of the search terms.
IDF (Inverse Document Frequency): Words that occur many times in a document, but also occur many times in many others, maybe irrelevant. IDF is a measure of how significant a term is in the entire corpus.
The TfidfVectorizer converts a collection of raw documents into a matrix of TF-IDF features.
In this project, we have used 20% data as a test set and 80% data as the training set by the train_tests_split lib.
For the decision tree classifier, import Decision tree classifier from sklearn.
Create a simple p ipeline and add countvectorizer and tfidftransformer to the model.
Apply the decision tree classifier model and get fitted in the model.
Now we can get the trained model, and test model and by processing get the predictions 
By using the lib imported in the start make the confusion matrix, and represent it virtually showing all results and predictions.
Graphs and matrices help in better understanding the data and predictions.
Code 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn import feature_extraction, linear_model, model_selection, preprocessing
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline


from google.colab import drive
drive.mount('/content/drive')
pathfake = "/content/drive/MyDrive/2467/Fake.csv"
fake = pd.read_csv(pathfake)
pathreal = '/content/drive/MyDrive/2467/True.csv'
true = pd.read_csv(pathreal)
fake.shape
true.shape
# Add flag to track fake and real
fake['target'] = 'fake'
true['target'] = 'true'
fake.head()
true.head()


# Concatenate dataframes
data = pd.concat([fake, true]).reset_index(drop = True)
data.shape
data.head(5)
data.tail(5)
# Shuffle the data
from sklearn.utils import shuffle
data = shuffle(data)
data = data.reset_index(drop=True)
# Check the data
data.head()


data.info()


# Removing the date and title(unnecessary data)
data.drop(["date"],axis=1,inplace=True)
data.head()
data.drop(["title"],axis=1,inplace=True)
data.head()
# Convert to lowercase
data['text'] = data['text'].apply(lambda x: x.lower())
data.head()


# Remove punctuation


import string


def punctuation_removal(text):
   all_list = [char for char in text if char not in string.punctuation]
   clean_str = ''.join(all_list)
   return clean_str


data['text'] = data['text'].apply(punctuation_removal)


# Removing stopwords
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop = stopwords.words('english')


data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))


data.head()
# How many articles per subject?
print(data.groupby(['subject'])['text'].count())
data.groupby(['subject'])['text'].count().plot(kind="bar")
plt.show()
# How many fake and real articles?
print(data.groupby(['target'])['text'].count())
data.groupby(['target'])['text'].count().plot(kind="bar")
plt.show()




# Function to plot the confusion matrix
from sklearn import metrics
import itertools


def plot_confusion_matrix(cm, classes,
                         normalize=False,
                         title='Confusion matrix',
                         cmap=plt.cm.Blues):
  
   plt.imshow(cm, interpolation='nearest', cmap=cmap)
   plt.title(title)
   plt.colorbar()
   tick_marks = np.arange(len(classes))
   plt.xticks(tick_marks, classes, rotation=45)
   plt.yticks(tick_marks, classes)


   if normalize:
       cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
       print("Normalized confusion matrix")
   else:
       print('Confusion matrix, without normalization')


   thresh = cm.max() / 2.
   for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
       plt.text(j, i, cm[i, j],
                horizontalalignment="center",
                color="white" if cm[i, j] > thresh else "black")


   plt.tight_layout()
   plt.ylabel('True label')
   plt.xlabel('Predicted label')


# Split the data
X_train,X_test,y_train,y_test = train_test_split(data['text'], data.target, test_size=0.2, random_state=42)
X_train.head()
y_train.head()


from sklearn.tree import DecisionTreeClassifier


# Vectorizing and applying TF-IDF
pipe = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                ('model', DecisionTreeClassifier(criterion= 'entropy',
                                          max_depth = 20,
                                          splitter='best',
                                          random_state=42))])
# Fitting the model
model = pipe.fit(X_train, y_train)


# Accuracy
prediction = model.predict(X_test)
print("accuracy: {}%".format(round(accuracy_score(y_test, prediction)*100,2)))
cm = metrics.confusion_matrix(y_test, prediction)
plot_confusion_matrix(cm, classes=['Fake', 'Real'])
Output 
Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount("/content/drive", force_remount=True).
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 44898 entries, 0 to 44897
Data columns (total 5 columns):
 #   Column   Non-Null Count  Dtype 
---  ------   --------------  ----- 
 0   title    44898 non-null  object
 1   text     44898 non-null  object
 2   subject  44898 non-null  object
 3   date     44898 non-null  object
 4   target   44898 non-null  object
dtypes: object(5)
memory usage: 1.7+ MB
[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
subject
Government News     1570
Middle-east          778
News                9050
US_News              783
left-news           4459
politics            6841
politicsNews       11272
worldnews          10145
Name: text, dtype: int64


target
fake    23481
true    21417
Name: text, dtype: int64


accuracy: 99.65%
Confusion matrix, without normalization





Conclusion
We learned to detect fake news with Python. We took a political dataset, implemented a TfidfVectorizer, initialized a PassiveAggressiveClassifier, and fit our model. We ended up obtaining an accuracy of 99.65% in magnitude. We have tried this fake news prediction with different models of predictions and the best accuracy we got was from the Decision Tree Model.
Decision Trees usually mimic human thinking ability while making a decision, so it is easy to understand.
The logic behind the decision tree can be easily understood because it shows a tree-like structure.
This model helps us to know how much fake news is there around us related to a particular subject and helps us to know what to believe and what not to believe. Nowadays fake news spread like wildfire and to keep ourselves safe from it and be updated these types of fake news detection models could be used.








